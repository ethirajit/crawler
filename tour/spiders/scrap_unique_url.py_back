# -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import Rule, CrawlSpider
from tour.items import uniqueUrlItem
from scrapy_splash import SplashRequest
from urllib.parse import urlencode, parse_qs
import tldextract
import json
import re
import string

class tourSpider(CrawlSpider):
    # The name of the spider
    name = "scrap_unique_url1"
    custom_settings = {
        'ITEM_PIPELINES': {
            'tour.pipelines.uniqueUrl': 300,
        }
    }

    def __init__(self, start_url=None, allowed_domain=None, *args, **kwargs):
        super(tourSpider, self).__init__(*args, **kwargs)
        self.start_urls = [start_url]
        self.allowed_domains = [allowed_domain]
        self.spash_url = '68.183.86.57:8050'
        self.main_deny_rule = []
        
    def start_requests(self):
        print ("++"*20)
        print (self.start_urls[0])
        print ("++"*20)
        yield SplashRequest(self.start_urls[0],
                            self.parse_items,
                            endpoint='render.html',
                            args={'wait': 0.5})

    def consecutive_requests(self, url):
        print ("**"*20)
        print (url)
        print ("**"*20)
        yield SplashRequest(url,
                            self.parse_items,
                            endpoint='render.html',
                            args={'wait': 0.5})

    # Method for parsing items
    def parse_items(self, response):
        # The list of items that are found on the particular page
        items = [] 
        self.parse_deny_rule = [] 
        links = LinkExtractor(deny=tuple(self.parse_deny_rule), canonicalize=True, unique=True).extract_links(response)
        
        pattern = re.compile('\/\/([^)]+)[\/\?\#]')
        # Now go through all the found links
        for link in links:
            # Check whether the domain of the URL of the link is allowed; so whether it is in one of the allowed domains
            is_allowed = False
            for allowed_domain in self.allowed_domains:
                if allowed_domain in link.url:
                    is_allowed = True

            # If it is allowed, create a new item and add it to the list of found items
            if is_allowed:
                # To allow URL only started with allowed domain and it's sub domain
                list_domain = tldextract.extract(link.url)
                domain_name = list_domain.domain + '.' + list_domain.suffix
                # Filtered offsite URL from redirecting URL, Assume this script will obey one url.
                if domain_name in self.allowed_domains[0]:
                    item = uniqueUrlItem()
                    #Get the first directory order in url
                    item['unique_url'] = str(pattern.findall(link.url)[0])
                    if item['unique_url'].split('.')[0] == 'www':
                        item['unique_url'] = '.'.join(item['unique_url'].split('.')[1:])
                        if item['unique_url'] in self.allowed_domains:
                            continue
                    else:
                        if item['unique_url'] in self.allowed_domains:
                            continue
                    if '?' in item['unique_url']:
                        item['unique_url'] = item['unique_url'].split('?')[0] #Remove the char after '?'
                    items.append(item)
                else:
                    pass

        #Remove dublicate in list of dict, It is a 2nd filter to reduce the Duplicat URL
        items = list({v['unique_url']:v for v in items}.values())

        #Generate custome RULE'S dynamically
        change_in_rule = False
        for item in items:
            if not item['unique_url'] in self.allowed_domains: 
                self.main_deny_rule.append(item['unique_url'])
                change_in_rule = True
                print ("++"*20)
                print ("Change in RULE")
                print ("++"*20)
                
        
        if change_in_rule:
            #
            #Generate Deny RULE'S dynamically
            #
            links = LinkExtractor(allow=tuple(self.main_deny_rule), canonicalize=True, unique=True).extract_links(response)
            for link in links:
                # Check whether the domain of the URL of the link is allowed; so whether it is in one of the allowed domains
                is_allowed = False
                for allowed_domain in self.allowed_domains:
                    if allowed_domain in link.url:
                        print ("=="*20)
                        print (link.url)
                        print ("=="*20)
                        self.consecutive_requests(link.url)

        # Return all the found items
        return items

def closed(self, spider):
        file_name = 'data/url/unique_'+self.allowed_domains[0]+'.json'
        file_obj = open(file_name, 'r')
        lines = file_obj.readlines()
        file_name = 'data/url/unique_simplified_'+self.allowed_domains[0]+'.json'
        all_url = list()
        tree_url = dict()
        for line in lines:
            line = json.loads(line)
            url_splits = line['unique_url'].split('/')
            url = ''
            for url_split in url_splits:
               if url == '':
                   url = url_split
               else:
                   url = url+'/'+url_split
               all_url.append(url)

        all_url = list(set(all_url))
        with open(file_name, 'w') as file_obj:
            for url in all_url:
                if '/' in url:
                    if 'level'+str(url.count('/')) in tree_url:
                        tree_url['level'+str(url.count('/'))].append(url)
                    else:
                        tree_url['level'+str(url.count('/'))] = list()
                        tree_url['level'+str(url.count('/'))].append(url)
                else:
                    if 'level0' in tree_url:
                        tree_url['level0'].append(url)
                    else:
                        tree_url['level0'] = list()
                        tree_url['level0'].append(url)
                         
            
            line = json.dumps(dict(tree_url)) 
            file_obj.write(line)
